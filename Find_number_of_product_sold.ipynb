{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilesh05/Anilesh/blob/main/Find_number_of_product_sold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Download and install hadoop*"
      ],
      "metadata": {
        "id": "BPJXWgx7hu12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UR7yMNueALI"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk\n",
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar fx hadoop-3.3.6.tar.gz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "!ln -s /content/hadoop-3.3.6/bin/* /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Create sales_data.txt file*"
      ],
      "metadata": {
        "id": "4sRNKOpdieYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sales_data.txt\n",
        "2024-04-01,Product_A,50,John,Dallas,Texas,USA,2023-01-01,2024-03-30,32.7767,-96.7970\n",
        "2024-04-01,Product_B,30,Jane,New York,New York,USA,2022-06-15,2024-03-31,40.7128,-74.0060\n",
        "2024-04-02,Product_A,50,John,Los Angeles,California,USA,2021-12-10,2024-04-01,34.0522,-118.2437\n",
        "2024-04-02,Product_C,20,Alice,Paris,,France,2023-05-20,2024-03-29,48.8566,2.3522\n",
        "2024-04-03,Product_A,50,John,London,,UK,2023-02-14,2024-04-02,51.5074,-0.1278\n",
        "2024-04-03,Product_B,30,Bob,Sydney,NSW,Australia,2022-09-30,2024-04-01,-33.8688,151.2093\n",
        "2024-04-03,Product_A,50,John,New Delhi,,India,2023-04-05,2024-03-31,28.6139,77.2090\n",
        "2024-04-04,Product_B,30,Bob,Tokyo,,Japan,2022-12-25,2024-03-30,35.6895,139.6917\n",
        "2024-04-05,Product_A,50,John,Beijing,,China,2023-01-20,2024-04-02,39.9042,116.4074\n",
        "2024-04-05,Product_C,20,Emma,Berlin,,Germany,2022-08-15,2024-03-29,52.5200,13.4050\n",
        "2024-04-06,Product_B,30,Bob,Rio de Janeiro,,Brazil,2022-11-10,2024-04-02,-22.9068,-43.1729\n",
        "2024-04-07,Product_A,50,John,Moscow,,Russia,2023-03-03,2024-04-01,55.7558,37.6176\n",
        "2024-04-08,Product_C,20,Emma,Mexico City,,Mexico,2022-07-05,2024-03-30,19.4326,-99.1332\n",
        "2024-04-09,Product_B,30,Bob,Istanbul,,Turkey,2022-10-01,2024-04-03,41.0082,28.9784\n",
        "2024-04-10,Product_A,50,John,Cairo,,Egypt,2023-06-10,2024-04-01,30.0444,31.2357\n",
        "2024-04-11,Product_B,30,Bob,Buenos Aires,,Argentina,2023-01-30,2024-03-31,-34.6037,-58.3816\n",
        "2024-04-12,Product_A,50,John,Lima,,Peru,2023-04-20,2024-04-03,-12.0464,-77.0428\n",
        "2024-04-13,Product_C,20,Emma,Bangkok,,Thailand,2022-12-01,2024-04-02,13.7563,100.5018\n",
        "2024-04-14,Product_B,30,Bob,Seoul,,South Korea,2023-02-14,2024-03-29,37.5665,126.9780\n",
        "2024-04-15,Product_A,50,John,Mumbai,,India,2023-07-10,2024-04-01,19.0760,72.8777\n",
        "2024-04-15,Product_B,30,Bob,Lagos,,Nigeria,2023-03-15,2024-04-03,6.5244,3.3792\n",
        "2024-04-16,Product_C,20,Emma,Kinshasa,,DR Congo,2023-01-10,2024-04-02,-4.4419,15.2663\n",
        "2024-04-17,Product_A,50,John,Karachi,,Pakistan,2023-08-25,2024-04-01,24.8607,67.0011\n",
        "2024-04-18,Product_B,30,Bob,Shanghai,,China,2023-04-30,2024-04-03,31.2304,121.4737\n",
        "2024-04-19,Product_A,50,John,Moscow,,Russia,2023-06-01,2024-04-01,55.7558,37.6176\n",
        "2024-04-20,Product_C,20,Emma,Beijing,,China,2023-02-18,2024-03-29,39.9042,116.4074\n",
        "2024-04-21,Product_B,30,Bob,Berlin,,Germany,2023-01-15,2024-04-02,52.5200,13.4050\n",
        "2024-04-22,Product_A,50,John,Rio de Janeiro,,Brazil,2023-05-05,2024-04-01,-22.9068,-43.1729\n",
        "2024-04-23,Product_C,20,Emma,Mexico City,,Mexico,2023-03-20,2024-04-03,19.4326,-99.1332\n",
        "2024-04-24,Product_B,30,Bob,Tokyo,,Japan,2023-07-30,2024-04-01,35.6895,139.6917\n",
        "2024-04-25,Product_A,50,John,Paris,,France,2023-10-10,2024-04-02,48.8566,2.3522\n",
        "2024-04-26,Product_C,20,Emma,Sydney,NSW,Australia,2023-04-25,2024-04-03,-33.8688,151.2093\n",
        "2024-04-27,Product_B,30,Bob,London,,UK,2023-01-05,2024-04-01,51.5074,-0.1278\n",
        "2024-04-28,Product_A,50,John,New York,New York,USA,2022-09-01,2024-04-02,40.7128,-74.0060\n",
        "2024-04-29,Product_B,30,Bob,Dallas,Texas,USA,2022-05-20,2024-04-03,32.7767,-96.7970\n",
        "2024-04-30,Product_C,20,Emma,Los Angeles,California,USA,2023-03-10,2024-04-01,34.0522,-118.2437"
      ],
      "metadata": {
        "id": "3IuzcKMOiaMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "# Input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # Remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # Split the line into fields\n",
        "    fields = line.split(',')\n",
        "\n",
        "    # Extract country and product\n",
        "    country = fields[6]\n",
        "    product = fields[1]\n",
        "\n",
        "    # Emit key-value pair for product and country\n",
        "    print('%s\\t%s' % (product, country))\n"
      ],
      "metadata": {
        "id": "qzPsPSMZh2x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "current_product = None\n",
        "current_country = None\n",
        "current_count = 0\n",
        "\n",
        "# Print column names\n",
        "print('{:15} {:<16} {:<15}'.format(\"product\",\"Country\",\"Count\"))\n",
        "print()\n",
        "\n",
        "\n",
        "# Input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # Remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # Parse the input we got from mapper.py\n",
        "    product, country = line.split('\\t', 1)\n",
        "\n",
        "    # If the current product and country are equal to the product and country we just read,\n",
        "    # increment the count for this product\n",
        "    if current_product == product and current_country == country:\n",
        "        current_count += 1\n",
        "    else:\n",
        "        # If it's not the first product or country (i.e., current_product is not None),\n",
        "        # print out the count for the previous product and country\n",
        "        if current_product:\n",
        "            print(f\"{current_product:<15} {current_country:<16} {current_count}\")\n",
        "        current_count = 1\n",
        "        current_product, current_country = product, country\n",
        "\n",
        "# Output the last product and country\n",
        "if current_product:\n",
        "      print(f\"{current_product:<15} {current_country:<16} {current_count}\")\n"
      ],
      "metadata": {
        "id": "bA4rjNS5iXXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mkdir input"
      ],
      "metadata": {
        "id": "gRt88Sde-lHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mv sales_data.txt input/"
      ],
      "metadata": {
        "id": "gCMg_d2AQgzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat input/sales_data.txt"
      ],
      "metadata": {
        "id": "RJHtXstEQx7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    -files mapper.py,reducer.py \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input input \\\n",
        "    -output output"
      ],
      "metadata": {
        "id": "HHmZWpZo-v7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/part-00000"
      ],
      "metadata": {
        "id": "-HL2Wbhy_LJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}