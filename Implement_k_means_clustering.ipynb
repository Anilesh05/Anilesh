{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilesh05/Anilesh/blob/main/Implement_k_means_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and install the apache hadoop"
      ],
      "metadata": {
        "id": "Zi9bK5z88-Pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp7HsNH56ldg"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk\n",
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar fx hadoop-3.3.6.tar.gz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "!ln -s /content/hadoop-3.3.6/bin/* /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Create centroids.txt file*"
      ],
      "metadata": {
        "id": "yxYjNtCmCtud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile centroids.txt\n",
        "1.0,2.0\n",
        "5.0,6.0\n",
        "10.0,11.0"
      ],
      "metadata": {
        "id": "ymK_CwxAg-x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Create points.txt file*"
      ],
      "metadata": {
        "id": "BAs7r8swCyB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile points.txt\n",
        "1.0,2.0\n",
        "2.0,3.0\n",
        "3.0,4.0\n",
        "4.0,5.0\n",
        "5.0,6.0\n",
        "6.0,7.0\n",
        "7.0,8.0\n",
        "8.0,9.0\n",
        "9.0,10.0\n",
        "10.0,11.0\n",
        "11.0,12.0\n",
        "12.0,13.0\n",
        "13.0,14.0\n",
        "14.0,15.0\n",
        "15.0,16.0"
      ],
      "metadata": {
        "id": "drJkRCHchBp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# Load centroids from file\n",
        "centroids = np.loadtxt('centroids.txt', delimiter=',')\n",
        "\n",
        "# Function to calculate Euclidean distance\n",
        "euclidean_distance = lambda point1, point2: np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "# Input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # Parse the data point\n",
        "    point = np.array([float(field) for field in line.strip().split(',')])\n",
        "    # Find the closest centroid for the data point\n",
        "    closest_centroid = min(range(len(centroids)), key=lambda i: euclidean_distance(point, centroids[i]))\n",
        "    # Emit the closest centroid ID and the data point\n",
        "    print(f'{closest_centroid}\\t{\",\".join(map(str, point))}')\n"
      ],
      "metadata": {
        "id": "WQRQ9hrFZeBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a dictionary to store centroids and their associated points\n",
        "centroids = {}\n",
        "\n",
        "# Read input from stdin\n",
        "for line in sys.stdin:\n",
        "    # Parse the centroid ID and point coordinates from the input line\n",
        "    centroid_id, point_str = line.strip().split('\\t', 1)\n",
        "    # Convert the point coordinates to a NumPy array\n",
        "    point = np.array(list(map(float, point_str.split(','))))\n",
        "    # Add the point to the list of points associated with the centroid ID\n",
        "    centroids.setdefault(centroid_id, []).append(point)\n",
        "\n",
        "# Calculate new centroids\n",
        "# Calculate new centroids\n",
        "print(\"Cluster ID\\tCluster Centroid(X,Y)\")\n",
        "print()\n",
        "\n",
        "for centroid_id, points in centroids.items():\n",
        "    # Compute the mean of all points associated with the centroid\n",
        "    new_centroid = np.mean(points, axis=0)\n",
        "    # Print the centroid ID and its new coordinates\n",
        "    print(f'{centroid_id}\\t\\t{\",\".join(map(str, new_centroid))}')\n"
      ],
      "metadata": {
        "id": "n42PsK4Ra4uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mkdir input"
      ],
      "metadata": {
        "id": "gRt88Sde-lHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mv points.txt input/"
      ],
      "metadata": {
        "id": "gCMg_d2AQgzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat centroids.txt"
      ],
      "metadata": {
        "id": "0OSjlJW5W3iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat input/points.txt"
      ],
      "metadata": {
        "id": "RJHtXstEQx7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    -files mapper.py,reducer.py \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input input \\\n",
        "    -output output"
      ],
      "metadata": {
        "id": "HHmZWpZo-v7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/part-00000"
      ],
      "metadata": {
        "id": "-HL2Wbhy_LJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}